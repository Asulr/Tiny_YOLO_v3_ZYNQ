void parse_net_options(network *net)
{
    net->batch = 1;
    net->learning_rate = 0.001f;
    net->momentum = 0.9f;
    net->decay = 0.0005f;
    int subdivs = 1;
    net->time_steps = 1;
    net->notruth = 0;
    net->batch /= subdivs;
    net->batch *= net->time_steps;
    net->subdivisions = subdivs;
    net->random = 0;

    net->adam = 0;

    net->h = 416;
    net->w = 416;
    net->c = 3;
    net->inputs = net->h * net->w * net->c;
    net->max_crop = net->w*2;
    net->min_crop = net->w;
    net->max_ratio = (float) net->max_crop / net->w;
    net->min_ratio = (float) net->min_crop / net->w;
    net->center = 0;
    net->clip = 0;

    net->angle = 0;
    net->aspect = 1;
    net->saturation = 1.5f;
    net->exposure = 1.5f;
    net->hue = 0.1f;

    net->policy = STEPS;
    net->burn_in = 1000;
    net->power = 4.0f;


    int *steps = calloc(2, sizeof(int));
    float *scales = calloc(2, sizeof(float));
    steps[0] = 400000;
    steps[1] = 450000;
    scales[0] = 0.1f;
    scales[1] = 0.1f;
    net->scales = scales;
    net->steps = steps;
    net->num_steps = 2;

    net->max_batches = 500200;
}

convolutional_layer parse_convolutional(int n, int size, int stride, int pad, int padding, int groups, ACTIVATION activation, int batch, int h, int w, int c, int batch_normalize, int binary, int xnor, int flipped, float dot)
{

    if(pad) padding = size/2;

    convolutional_layer layer = make_convolutional_layer(batch,h,w,c,n,groups,size,stride,padding,activation, batch_normalize, binary, xnor, params.net->adam);
    layer.flipped = flipped;
    layer.dot = dot;

    return layer;
}

maxpool_layer parse_maxpool(int stride, int size, int padding, int batch, int h, int w, int c)
{
    maxpool_layer layer = make_maxpool_layer(batch,h,w,c,size,stride,padding);
    return layer;
}

layer parse_yolo(int classes, int total, int num, int *mask, int batch, int w, int h, int max, float jitter, float ignore_thresh, float truth_thresh, int random, float *anchors)
{

    layer l = make_yolo_layer(params.batch, params.w, params.h, num, total, mask, classes);

    l.max_boxes = max;
    l.jitter = jitter;
    l.ignore_thresh = ignore_thresh;
    l.truth_thresh = truth_thresh;
    l.random = random;

    for(int i = 0; i < 2*total; ++i){
        l.biases[i] = anchors[i];
    }

    return l;
}

route_layer parse_route(network *net, int batch, int n, int *layers, int *sizes)
{

    route_layer layer = make_route_layer(batch, n, layers, sizes);

    convolutional_layer first = net->layers[layers[0]];
    layer.out_w = first.out_w;
    layer.out_h = first.out_h;
    layer.out_c = first.out_c;
    for(int i = 1; i < n; ++i){
        int index = layers[i];
        convolutional_layer next = net->layers[index];
        if(next.out_w == first.out_w && next.out_h == first.out_h){
            layer.out_c += next.out_c;
        }else{
            layer.out_h = layer.out_w = layer.out_c = 0;
        }
    }

    return layer;
}

layer parse_upsample(int stride, int batch, int w, int h, int c, int stride, float scale)
{

    int stride = option_find_int(options, "stride",2);
    layer l = make_upsample_layer(params.batch, params.w, params.h, params.c, stride);
    l.scale = option_find_float_quiet(options, "scale", 1);
    return l;
}

network *parse_network_cfg()
{
	network *net = make_network(24); // network.c
    net->gpu_index = gpu_index;
    size_params params;

    parse_net_options(net);

    params.h = net->h;
    params.w = net->w;
    params.c = net->c;
    params.inputs = net->inputs;
    params.batch = net->batch;
    params.time_steps = net->time_steps;
    params.net = net;

    size_t workspace_size = 0;
    int count = 0;

    //layer 0 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 16;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 1 - max
    {
		params.index = count;
		layer l = {0};

		int stride = 2;
		int size = 2;
		int padding = size-1;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;

		l = parse_maxpool(stride,size,padding,batch,h,w,c);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 2 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 32;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 3 - max
    {
		params.index = count;
		layer l = {0};

		int stride = 2;
		int size = 2;
		int padding = size-1;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;

		l = parse_maxpool(stride,size,padding,batch,h,w,c);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 4 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 64;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 5 - max
    {
		params.index = count;
		layer l = {0};

		int stride = 2;
		int size = 2;
		int padding = size-1;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;

		l = parse_maxpool(stride,size,padding,batch,h,w,c);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 6 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 128;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 7 - max
    {
		params.index = count;
		layer l = {0};

		int stride = 2;
		int size = 2;
		int padding = size-1;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;

		l = parse_maxpool(stride,size,padding,batch,h,w,c);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 8 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 256;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 9 - max
    {
		params.index = count;
		layer l = {0};

		int stride = 2;
		int size = 2;
		int padding = size-1;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;

		l = parse_maxpool(stride,size,padding,batch,h,w,c);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 10 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 512;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 11 - max
    {
		params.index = count;
		layer l = {0};

		int stride = 2;
		int size = 1;
		int padding = size-1;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;

		l = parse_maxpool(stride,size,padding,batch,h,w,c);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 12 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 1024;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 13 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 256;
		int size = 1;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 14 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 512;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 15 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 255;
		int size = 1;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LINEAR;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 0;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 16 - yolo
    {
		params.index = count;
		layer l = {0};

		int classes = 80;
		int total = 6;
		int num = 3;
		int mask[3] = {3,4,5};
		int batch = params.batch;
		int w = params.w;
		int h = params.h;
		int max = 90;
		float jitter = 0.3f;
		float ignore_thresh = 0.7f;
		float truth_thresh = 1.0f;
		int random = 1;
		float anchors[12] = {10,14,23,27,37,58,81,82,135,169,344,319};

		l = parse_yolo(classes,total,num,&mask[0],batch,w,h,max,jitter,ignore_thresh,truth_thresh,random,&anchors[0]);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 17 - route
    {
		params.index = count;
		layer l = {0};

		int batch = params.batch;
		int n = 1;
		int layers[1] = {params.index-4};
		int sizes[1] = {net->layers[params.index-4].outputs};

		l = parse_route(net,batch,n,&layers[0],&sizes[0]);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 18 - conv
     {
 		params.index = count;
 		layer l = {0};

		int n = 128;
		int size = 1;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
 		l.clip = net->clip;
 		net->layers[count] = l;
 		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
 		++count;
 		params.h = l.out_h;
 		params.w = l.out_w;
 		params.c = l.out_c;
 		params.inputs = l.outputs;
     }

     //layer 19 - upsample
      {
  		params.index = count;
  		layer l = {0};
  		l = parse_upsample();
  		l.clip = net->clip;
  		net->layers[count] = l;
  		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
  		++count;
  		params.h = l.out_h;
  		params.w = l.out_w;
  		params.c = l.out_c;
  		params.inputs = l.outputs;
      }

      //layer 20 - route
      {
  		params.index = count;
  		layer l = {0};

		int batch = params.batch;
		int n = 2;
		int layers[2] = {params.index-1, 8};
		int sizes[2] = {net->layers[params.index-1].outputs, net->layers[8].outputs};

		l = parse_route(net,batch,n,&layers[0],&sizes[0]);
  		l.clip = net->clip;
  		net->layers[count] = l;
  		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
  		++count;
  		params.h = l.out_h;
  		params.w = l.out_w;
  		params.c = l.out_c;
  		params.inputs = l.outputs;
      }

      //layer 21 - conv
       {
   		params.index = count;
   		layer l = {0};


		int n = 256;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
   		l.clip = net->clip;
   		net->layers[count] = l;
   		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
   		++count;
   		params.h = l.out_h;
   		params.w = l.out_w;
   		params.c = l.out_c;
   		params.inputs = l.outputs;
       }

       //layer 22 - conv
        {
    		params.index = count;
    		layer l = {0};

    		int n = 255;
    		int size = 1;
    		int stride = 1;
    		int pad = 1;
    		int padding = 0;
    		int groups = 1;
    		ACTIVATION activation = LINEAR;
    		int batch = params.batch;
    	    int h = params.h;
    	    int w = params.w;
    	    int c = params.c;
    	    int batch_normalize = 0;
    	    int binary = 0;
    	    int xnor = 0;
    	    int flipped = 0;
    	    float dot = 0.0f;

    		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
    		l.clip = net->clip;
    		net->layers[count] = l;
    		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
    		++count;
    		params.h = l.out_h;
    		params.w = l.out_w;
    		params.c = l.out_c;
    		params.inputs = l.outputs;
        }

        //layer 23 - yolo
        {
    		params.index = count;
    		layer l = {0};

    		int classes = 80;
    		int total = 6;
    		int num = 3;
    		int mask[3] = {0,1,2};
    		int batch = params.batch;
    		int w = params.w;
    		int h = params.h;
    		int max = 90;
    		float jitter = 0.3f;
    		float ignore_thresh = 0.7f;
    		float truth_thresh = 1.0f;
    		int random = 1;
    		float anchors[12] = {10,14,23,27,37,58,81,82,135,169,344,319};

    		l = parse_yolo(classes,total,num,&mask[0],batch,w,h,max,jitter,ignore_thresh,truth_thresh,random,&anchors[0]);
    		l.clip = net->clip;
    		net->layers[count] = l;
    		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
    		++count;
        }

        layer out = get_network_output_layer(net);
        net->outputs = out.outputs;
        net->truths = out.outputs;
        if(net->layers[net->n-1].truths) net->truths = net->layers[net->n-1].truths;
        net->output = out.output;
        net->input = calloc(net->inputs*net->batch, sizeof(float));
        net->truth = calloc(net->truths*net->batch, sizeof(float));

        if(workspace_size)
        {
        	net->workspace = calloc(1, workspace_size);
        }

        return net;
}
