#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <assert.h>

#include "activation_layer.h"
#include "logistic_layer.h"
#include "l2norm_layer.h"
#include "activations.h"
#include "avgpool_layer.h"
#include "batchnorm_layer.h"
#include "blas.h"
#include "connected_layer.h"
#include "deconvolutional_layer.h"
#include "convolutional_layer.h"
#include "cost_layer.h"
#include "crnn_layer.h"
#include "crop_layer.h"
#include "detection_layer.h"
#include "dropout_layer.h"
#include "gru_layer.h"
#include "list.h"
#include "local_layer.h"
#include "maxpool_layer.h"
#include "normalization_layer.h"
#include "option_list.h"
#include "parser.h"
#include "region_layer.h"
#include "yolo_layer.h"
#include "iseg_layer.h"
#include "reorg_layer.h"
#include "rnn_layer.h"
#include "route_layer.h"
#include "upsample_layer.h"
#include "shortcut_layer.h"
#include "softmax_layer.h"
#include "lstm_layer.h"
#include "utils.h"

void parse_net_options(network *net)
{
    net->batch = 1;
    net->learning_rate = 0.001f;
    net->momentum = 0.9f;
    net->decay = 0.0005f;
    int subdivs = 1;
    net->time_steps = 1;
    net->notruth = 0;
    net->batch /= subdivs;
    net->batch *= net->time_steps;
    net->subdivisions = subdivs;
    net->random = 0;

    net->adam = 0;

    net->h = 416;
    net->w = 416;
    net->c = 3;
    net->inputs = net->h * net->w * net->c;
    net->max_crop = net->w*2;
    net->min_crop = net->w;
    net->max_ratio = (float) net->max_crop / net->w;
    net->min_ratio = (float) net->min_crop / net->w;
    net->center = 0;
    net->clip = 0;

    net->angle = 0;
    net->aspect = 1;
    net->saturation = 1.5f;
    net->exposure = 1.5f;
    net->hue = 0.1f;

    net->policy = STEPS;
    net->burn_in = 1000;
    net->power = 4.0f;


    int *steps = calloc(2, sizeof(int));
    float *scales = calloc(2, sizeof(float));
    steps[0] = 400000;
    steps[1] = 450000;
    scales[0] = 0.1f;
    scales[1] = 0.1f;
    net->scales = scales;
    net->steps = steps;
    net->num_steps = 2;

    net->max_batches = 500200;
}

convolutional_layer parse_convolutional(int n, int size, int stride, int pad, int padding, int groups, ACTIVATION activation, int batch, int h, int w, int c, int batch_normalize, int binary, int xnor, int flipped, float dot)
{

    if(pad) padding = size/2;

    convolutional_layer layer = make_convolutional_layer(batch,h,w,c,n,groups,size,stride,padding,activation, batch_normalize, binary, xnor, params.net->adam);
    layer.flipped = flipped;
    layer.dot = dot;

    return layer;
}

maxpool_layer parse_maxpool(int stride, int size, int padding, int batch, int h, int w, int c)
{
    maxpool_layer layer = make_maxpool_layer(batch,h,w,c,size,stride,padding);
    return layer;
}

layer parse_yolo(int classes, int total, int num, int *mask, int batch, int w, int h, int max, float jitter, float ignore_thresh, float truth_thresh, int random, float *anchors)
{

    layer l = make_yolo_layer(params.batch, params.w, params.h, num, total, mask, classes);

    l.max_boxes = max;
    l.jitter = jitter;
    l.ignore_thresh = ignore_thresh;
    l.truth_thresh = truth_thresh;
    l.random = random;

    for(int i = 0; i < 2*total; ++i){
        l.biases[i] = anchors[i];
    }

    return l;
}

route_layer parse_route(network *net, int batch, int n, int *layers, int *sizes)
{

    route_layer layer = make_route_layer(batch, n, layers, sizes);

    convolutional_layer first = net->layers[layers[0]];
    layer.out_w = first.out_w;
    layer.out_h = first.out_h;
    layer.out_c = first.out_c;
    for(int i = 1; i < n; ++i){
        int index = layers[i];
        convolutional_layer next = net->layers[index];
        if(next.out_w == first.out_w && next.out_h == first.out_h){
            layer.out_c += next.out_c;
        }else{
            layer.out_h = layer.out_w = layer.out_c = 0;
        }
    }

    return layer;
}

layer parse_upsample(int stride, int batch, int w, int h, int c, float scale)
{
    layer l = make_upsample_layer(batch, w, h, c, stride);
    l.scale = scale;
    return l;
}

network *parse_network_cfg(void)
{
	network *net = make_network(24); // network.c
    net->gpu_index = gpu_index;
    size_params params;

    parse_net_options(net);

    params.h = net->h;
    params.w = net->w;
    params.c = net->c;
    params.inputs = net->inputs;
    params.batch = net->batch;
    params.time_steps = net->time_steps;
    params.net = net;

    size_t workspace_size = 0;
    int count = 0;

    //layer 0 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 16;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 1 - max
    {
		params.index = count;
		layer l = {0};

		int stride = 2;
		int size = 2;
		int padding = size-1;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;

		l = parse_maxpool(stride,size,padding,batch,h,w,c);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 2 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 32;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 3 - max
    {
		params.index = count;
		layer l = {0};

		int stride = 2;
		int size = 2;
		int padding = size-1;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;

		l = parse_maxpool(stride,size,padding,batch,h,w,c);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 4 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 64;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 5 - max
    {
		params.index = count;
		layer l = {0};

		int stride = 2;
		int size = 2;
		int padding = size-1;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;

		l = parse_maxpool(stride,size,padding,batch,h,w,c);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 6 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 128;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 7 - max
    {
		params.index = count;
		layer l = {0};

		int stride = 2;
		int size = 2;
		int padding = size-1;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;

		l = parse_maxpool(stride,size,padding,batch,h,w,c);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 8 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 256;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 9 - max
    {
		params.index = count;
		layer l = {0};

		int stride = 2;
		int size = 2;
		int padding = size-1;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;

		l = parse_maxpool(stride,size,padding,batch,h,w,c);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 10 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 512;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 11 - max
    {
		params.index = count;
		layer l = {0};

		int stride = 2;
		int size = 1;
		int padding = size-1;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;

		l = parse_maxpool(stride,size,padding,batch,h,w,c);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 12 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 1024;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 13 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 256;
		int size = 1;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 14 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 512;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 15 - conv
    {
		params.index = count;
		layer l = {0};

		int n = 255;
		int size = 1;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LINEAR;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 0;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 16 - yolo
    {
		params.index = count;
		layer l = {0};

		int classes = 80;
		int total = 6;
		int num = 3;
		int mask[3] = {3,4,5};
		int batch = params.batch;
		int w = params.w;
		int h = params.h;
		int max = 90;
		float jitter = 0.3f;
		float ignore_thresh = 0.7f;
		float truth_thresh = 1.0f;
		int random = 1;
		float anchors[12] = {10,14,23,27,37,58,81,82,135,169,344,319};

		l = parse_yolo(classes,total,num,&mask[0],batch,w,h,max,jitter,ignore_thresh,truth_thresh,random,&anchors[0]);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 17 - route
    {
		params.index = count;
		layer l = {0};

		int batch = params.batch;
		int n = 1;
		int layers[1] = {params.index-4};
		int sizes[1] = {net->layers[params.index-4].outputs};

		l = parse_route(net,batch,n,&layers[0],&sizes[0]);
		l.clip = net->clip;
		net->layers[count] = l;
		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
		++count;
		params.h = l.out_h;
		params.w = l.out_w;
		params.c = l.out_c;
		params.inputs = l.outputs;
    }

    //layer 18 - conv
     {
 		params.index = count;
 		layer l = {0};

		int n = 128;
		int size = 1;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
 		l.clip = net->clip;
 		net->layers[count] = l;
 		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
 		++count;
 		params.h = l.out_h;
 		params.w = l.out_w;
 		params.c = l.out_c;
 		params.inputs = l.outputs;
     }

     //layer 19 - upsample
      {
  		params.index = count;
  		layer l = {0};

  		int stride = 2;
  		int batch = params.batch;
  		int w = params.w;
  		int h = params.h;
  		int c = params.c;
  		float scale = 1.0f;

  		l = parse_upsample(int stride, int batch, int w, int h, int c, float scale);
  		l.clip = net->clip;
  		net->layers[count] = l;
  		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
  		++count;
  		params.h = l.out_h;
  		params.w = l.out_w;
  		params.c = l.out_c;
  		params.inputs = l.outputs;
      }

      //layer 20 - route
      {
  		params.index = count;
  		layer l = {0};

		int batch = params.batch;
		int n = 2;
		int layers[2] = {params.index-1, 8};
		int sizes[2] = {net->layers[params.index-1].outputs, net->layers[8].outputs};

		l = parse_route(net,batch,n,&layers[0],&sizes[0]);
  		l.clip = net->clip;
  		net->layers[count] = l;
  		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
  		++count;
  		params.h = l.out_h;
  		params.w = l.out_w;
  		params.c = l.out_c;
  		params.inputs = l.outputs;
      }

      //layer 21 - conv
       {
   		params.index = count;
   		layer l = {0};


		int n = 256;
		int size = 3;
		int stride = 1;
		int pad = 1;
		int padding = 0;
		int groups = 1;
		ACTIVATION activation = LEAKY;
		int batch = params.batch;
	    int h = params.h;
	    int w = params.w;
	    int c = params.c;
	    int batch_normalize = 1;
	    int binary = 0;
	    int xnor = 0;
	    int flipped = 0;
	    float dot = 0.0f;

		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
   		l.clip = net->clip;
   		net->layers[count] = l;
   		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
   		++count;
   		params.h = l.out_h;
   		params.w = l.out_w;
   		params.c = l.out_c;
   		params.inputs = l.outputs;
       }

       //layer 22 - conv
        {
    		params.index = count;
    		layer l = {0};

    		int n = 255;
    		int size = 1;
    		int stride = 1;
    		int pad = 1;
    		int padding = 0;
    		int groups = 1;
    		ACTIVATION activation = LINEAR;
    		int batch = params.batch;
    	    int h = params.h;
    	    int w = params.w;
    	    int c = params.c;
    	    int batch_normalize = 0;
    	    int binary = 0;
    	    int xnor = 0;
    	    int flipped = 0;
    	    float dot = 0.0f;

    		l = parse_convolutional(n,size,stride,pad,padding,groups,activation,batch,h,w,c,batch_normalize,binary,xnor,flipped,dot);
    		l.clip = net->clip;
    		net->layers[count] = l;
    		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
    		++count;
    		params.h = l.out_h;
    		params.w = l.out_w;
    		params.c = l.out_c;
    		params.inputs = l.outputs;
        }

        //layer 23 - yolo
        {
    		params.index = count;
    		layer l = {0};

    		int classes = 80;
    		int total = 6;
    		int num = 3;
    		int mask[3] = {0,1,2};
    		int batch = params.batch;
    		int w = params.w;
    		int h = params.h;
    		int max = 90;
    		float jitter = 0.3f;
    		float ignore_thresh = 0.7f;
    		float truth_thresh = 1.0f;
    		int random = 1;
    		float anchors[12] = {10,14,23,27,37,58,81,82,135,169,344,319};

    		l = parse_yolo(classes,total,num,&mask[0],batch,w,h,max,jitter,ignore_thresh,truth_thresh,random,&anchors[0]);
    		l.clip = net->clip;
    		net->layers[count] = l;
    		if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;
    		++count;
        }

        layer out = get_network_output_layer(net);
        net->outputs = out.outputs;
        net->truths = out.outputs;
        if(net->layers[net->n-1].truths) net->truths = net->layers[net->n-1].truths;
        net->output = out.output;
        net->input = calloc(net->inputs*net->batch, sizeof(float));
        net->truth = calloc(net->truths*net->batch, sizeof(float));

        if(workspace_size)
        {
        	net->workspace = calloc(1, workspace_size);
        }

        return net;
}

void load_convolutional_weights(layer l, Layer_Weight_File *conv_weight_file_ptr)
{
    if(l.binary){
        //load_convolutional_weights_binary(l, fp);
        //return;
    }
    if(l.numload) l.n = l.numload;
    int num = l.c/l.groups*l.n*l.size*l.size;
    l.biases = conv_weight_file_ptr->biases;
    if (l.batch_normalize && (!l.dontloadscales)){
    	l.scales = conv_weight_file_ptr->scales;
        l.rolling_mean = conv_weight_file_ptr->rolling_mean;
        l.rolling_variance = conv_weight_file_ptr->rolling_variance;
    }
    l.weights = conv_weight_file_ptr->weights;
    //if(l.c == 3) scal_cpu(num, 1./256, l.weights, 1);
    //if (l.binary) binarize_weights(l.weights, l.n, l.c*l.size*l.size, l.weights);
}

void load_weights_upto(network *net, int start, int cutoff, Layer_Weight_File conv_weight_file[13])
{
    fprintf(stderr, "Loading weights ...");
    fflush(stdout);

    int major = 0;
    int minor = 2;
    int revision = 0;

    net->seen = 32013312;

    int i;
    int j = 0;
    for(i = start; i < net->n && i < cutoff; ++i){
        layer l = net->layers[i];
        if (l.dontload) continue;
        if(l.type == CONVOLUTIONAL){
            load_convolutional_weights(l, &conv_weight_file[j]);
            j++;
        }
    }
    fprintf(stderr, "Done!\n");
}

void load_weights(network *net)
{
    Layer_Weight_File conv_weight_file[13];

    conv_weight_file[0].biases           = conv_0_biases;
    conv_weight_file[0].scales           = conv_0_scales;
    conv_weight_file[0].rolling_mean     = conv_0_rolling_mean;
    conv_weight_file[0].rolling_variance = conv_0_rolling_variance;
    conv_weight_file[0].weights          = conv_0_weights;

    conv_weight_file[1].biases           = conv_1_biases;
    conv_weight_file[1].scales           = conv_1_scales;
    conv_weight_file[1].rolling_mean     = conv_1_rolling_mean;
    conv_weight_file[1].rolling_variance = conv_1_rolling_variance;
    conv_weight_file[1].weights          = conv_1_weights;

    conv_weight_file[2].biases           = conv_2_biases;
    conv_weight_file[2].scales           = conv_2_scales;
    conv_weight_file[2].rolling_mean     = conv_2_rolling_mean;
    conv_weight_file[2].rolling_variance = conv_2_rolling_variance;
    conv_weight_file[2].weights          = conv_2_weights;

    conv_weight_file[3].biases           = conv_3_biases;
    conv_weight_file[3].scales           = conv_3_scales;
    conv_weight_file[3].rolling_mean     = conv_3_rolling_mean;
    conv_weight_file[3].rolling_variance = conv_3_rolling_variance;
    conv_weight_file[3].weights          = conv_3_weights;

    conv_weight_file[4].biases           = conv_4_biases;
    conv_weight_file[4].scales           = conv_4_scales;
    conv_weight_file[4].rolling_mean     = conv_4_rolling_mean;
    conv_weight_file[4].rolling_variance = conv_4_rolling_variance;
    conv_weight_file[4].weights          = conv_4_weights;

    conv_weight_file[5].biases           = conv_5_biases;
    conv_weight_file[5].scales           = conv_5_scales;
    conv_weight_file[5].rolling_mean     = conv_5_rolling_mean;
    conv_weight_file[5].rolling_variance = conv_5_rolling_variance;
    conv_weight_file[5].weights          = conv_5_weights;

    conv_weight_file[6].biases           = conv_6_biases;
    conv_weight_file[6].scales           = conv_6_scales;
    conv_weight_file[6].rolling_mean     = conv_6_rolling_mean;
    conv_weight_file[6].rolling_variance = conv_6_rolling_variance;
    conv_weight_file[6].weights          = conv_6_weights;

    conv_weight_file[7].biases           = conv_7_biases;
    conv_weight_file[7].scales           = conv_7_scales;
    conv_weight_file[7].rolling_mean     = conv_7_rolling_mean;
    conv_weight_file[7].rolling_variance = conv_7_rolling_variance;
    conv_weight_file[7].weights          = conv_7_weights;

    conv_weight_file[8].biases           = conv_8_biases;
    conv_weight_file[8].scales           = conv_8_scales;
    conv_weight_file[8].rolling_mean     = conv_8_rolling_mean;
    conv_weight_file[8].rolling_variance = conv_8_rolling_variance;
    conv_weight_file[8].weights          = conv_8_weights;

    conv_weight_file[9].biases           = conv_9_biases;
    conv_weight_file[9].scales           = conv_9_scales;
    conv_weight_file[9].rolling_mean     = conv_9_rolling_mean;
    conv_weight_file[9].rolling_variance = conv_9_rolling_variance;
    conv_weight_file[9].weights          = conv_9_weights;

    conv_weight_file[10].biases           = conv_10_biases;
    conv_weight_file[10].scales           = conv_10_scales;
    conv_weight_file[10].rolling_mean     = conv_10_rolling_mean;
    conv_weight_file[10].rolling_variance = conv_10_rolling_variance;
    conv_weight_file[10].weights          = conv_10_weights;

    conv_weight_file[11].biases           = conv_11_biases;
    conv_weight_file[11].scales           = conv_11_scales;
    conv_weight_file[11].rolling_mean     = conv_11_rolling_mean;
    conv_weight_file[11].rolling_variance = conv_11_rolling_variance;
    conv_weight_file[11].weights          = conv_11_weights;

    conv_weight_file[12].biases           = conv_12_biases;
    conv_weight_file[12].scales           = conv_12_scales;
    conv_weight_file[12].rolling_mean     = conv_12_rolling_mean;
    conv_weight_file[12].rolling_variance = conv_12_rolling_variance;
    conv_weight_file[12].weights          = conv_12_weights;

    load_weights_upto(net, 0, net->n, &conv_weight_file[0]);
}
